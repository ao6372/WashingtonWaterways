This is a guide to show short descriptions of code in this project and
where to edit certain parameters for further study.

NetCDFDataClean.py
This file takes NetCDF files and turns them into dataframes to be
compatible for the rest of the computation/models. It converts data from the UW
atmospheric science database.

line 6  - main function has a filepath that you can change to clean netcdf
          on different locations saved
line 19 -the first file I worked with included dates that were from 1950 so
          I filtered for 1985 and on. You can change the dates or get rid of
          this line if it is unnecessary.
line 81 -This takes the top ONE value per year in a dataframe. If you want to
          consider a different amount of values per water year, I have left code
          example for 30 values per water year.

scraping UW database portion
line 112-this is an example of baselinks. You can change baselinks depending
          on the location of the model in URL form.
          The linklist portion has a parse to start from 5 and on because that
          page started to show link information after the 5th item in the list.
          If you are using this for another baselink, consider checking the
          linklist parse. The data also is limited to 198510 and beyond (change
          if necessary).

Model.py
This file generates a GEV based on calculated shape, scale, and location.
The method for these parameters is documented in the reference_publications
directory with a paper by Hoskins.

line 17- this function takes the past 30 years of data for calculation of the
          GEV of interest. For example the GEV for 2016 depends on data from 1986-2016.
          You can change line 25 to have a different window for years. We chose the
          past 30 years because it has proven to make a GEV that is representative.
line 78 - A random sample of 20 out of 30 data points is bootstrapped for each
          GEV model (this number was tuned to give us a feel for a distribution
          and reduce computation time)
line 94-this function produces 1000 GEV samples. We tuned it to run 1000 samples
        because it generated a wide enough distribution to study and going more than
        1000 samples would add more computation time.
line 126- this function makes ratios over the current year. It is important to pick
          the right base year. In the future consider, adding in script that only
          lets you calculate from your actual base year. (For our study we used 2015)
          Data analysis shows that within 2015-2018 there aren't large changes in
          ratios.
line 142- this function is used to extract the quantiles from 1000.
          We plot and keep .05 percentile for visualization but every time this is run, the
          .5 and .95 data is also generated. This eventually contributes to the download
          portion of the website to give confidence bound estimation.
line 162-this line makes sure that the data presented and returned is raised to the appropriate
        factor which is region specific and generated by the location_parameters.py


multiplemodels.py
This program depends on the model.py file. It takes in the cleaned csv files from
NetCDFDataClean and references them to generate a sample set for each model.
There are 6 models referenced in this study and each model produces
1000 GEVs for each year and the five percentile portion of this distribution is saved
for graphing. The results from all the models are considered equally likely
and we use their results to calculate probability of a certain stream width occuring.

This script can generate all the GEV samples for each reference file shown in two
examples in line 75 and line 94. Save the paths accordingly and execute the make_multimodel_plotdataA1
to generate the data needed for the website to run.

Downloaddata is the dataframe (easily converted to csv for CIG purposes) that
shows the .05,.5, .95 percentile of the of all the models.



location_parameters.py
This file references 'VIC_Castro_Regions.csv'. This file has the latitude and Longitude
combinations and their associated regions which determines the beta and TBFW variables.

Note that if the latitude and longitude of interest is not in the reference file
'VIC_Castro_Regions.csv' , the program logs it in a missing.log file.

For uploading data to SQL or generating csv files that use location_parameters.py
please make sure all the latitude and longitudes of interest are referenced in the
the 'VIC_Castro_Regions.csv' or the replacement file for this.

slloader.py
This file uploads to a SQL database (in my case I used an Amazon RDS instance).
The database will run code for the latitudes and longitudes of interest and produce
a table with ratio, model, latitude, longitude. The SQL queries that are useful are
stored in queries.sql.

line 80- I saved my information on my bash profile. This exact syntax needs to be updated
        to match the user's bash profile.

Other notes
The code works by reading in NetCDF files which have no label for the dimensions associated
with each volumetric flow rate value (lat, lon, time). I used Pandas to get these dimensions
labeled with the volumetric flow rate values.
